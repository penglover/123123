<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Wellcome to IT Blog</title>
  <subtitle>Penglover&#39;s Software house</subtitle>
  <link href="/feed.xml" rel="self"/>
  
  <link href="https://penglover.github.io/"/>
  <updated>2017-01-15T08:48:26.000Z</updated>
  <id>https://penglover.github.io/</id>
  
  <author>
    <name>Myeongsoo Kim</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>머신러닝 코드 최적화하기 by rate, overfitting, regularization</title>
    <link href="https://penglover.github.io/2017/01/15/how-can-upgrade-mlcode/"/>
    <id>https://penglover.github.io/2017/01/15/how-can-upgrade-mlcode/</id>
    <published>2017-01-15T08:07:38.000Z</published>
    <updated>2017-01-15T08:48:26.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="머신러닝-코드-최적화하기"><a href="#머신러닝-코드-최적화하기" class="headerlink" title="머신러닝 코드 최적화하기"></a>머신러닝 코드 최적화하기</h1><p><strong>rate</strong><br>항상 우리는 머신러닝 코드를 짤때 learning rate라는 값을 줍니다.<br>gradient descent알고리즘을 쓸 때에 업데이트 정도를 조절해주는데요.<br>우리는 이 learning rate 를 조절함으로써 코드를 최적화 할 수 있습니다.<br>너무 작은값을 주면 어떻게 될까요?<br>또는 너무 큰값을 주면 어떻게 될까요?<br><img src="https://penglover.github.io/images/learningrate.png" alt="compare_learningrate"><br>위의 그림처럼 되어서 최적화가 안됩니다.<br>해결방안은?<br>print로 꾸준히 cost함수의 변화를 체크하는 것이 최선입니다.</p>
<p><strong>overfitting</strong><br>overfitting이라 하면 주로 test data가 적어서 일어납니다.<br>무리하게 적은 데이터로 fitting을 하려다보니 이상해 지는 것이지요.<br>또는 weight간의 값이 편차가 심하거나 해도 마찬가지로 일어납니다.<br>그래프가 구부러져 보인다고 해서 ‘구부러졌다’라고 표현하기도 합니다.<br>표준정규분포를 혹시 아시나요?<br>z = (x-평균)/표준편차<br>고등학교 수학시간에 한번 보셨을 겁니다.<br>이것을 이용해서 weight간의 격차를 일반화해줌으로서 해결하는 경우가 대부분입니다.<br>또는 regularization을 통해 해결할 수 있는데요.</p>
<p><strong>regularization</strong><br>기존의 우리가 cost를 구하는 개념에서 weight의 제곱을 또 추가한것을 구하는 개념입니다.<br>gradient descent 알고리즘을 사용한다면 자동으로 weight의 제곱까지 고려해서 minimize를 시킬 것이고 그에따라서 자동으로 weight의 값들도 하향 평준화 시켜서 값을 낮춤으로써 구부러짐을 방지해주지요.<br>구부러짐이 뭐지 하는생각이 들 수 있습니다.<br><img src="https://penglover.github.io/images/regularization" alt="regularization"><br><img src="http://cs231n.github.io/assets/nn1/reg_strengths.jpeg" alt="overfitting_example"></p>
<p>위에서 보이듯 람다값이 작을수록 weight 값을 신경쓰지 않겠다는 것입니다.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;머신러닝-코드-최적화하기&quot;&gt;&lt;a href=&quot;#머신러닝-코드-최적화하기&quot; class=&quot;headerlink&quot; title=&quot;머신러닝 코드 최적화하기&quot;&gt;&lt;/a&gt;머신러닝 코드 최적화하기&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;rate&lt;/strong&gt;&lt;br&gt;항
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="algorithm" scheme="https://penglover.github.io/categories/ML/algorithm/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="algorithm" scheme="https://penglover.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우에서 softmax 메서드를 사용해 봅시다!</title>
    <link href="https://penglover.github.io/2017/01/15/tensorflow-softmaxC/"/>
    <id>https://penglover.github.io/2017/01/15/tensorflow-softmaxC/</id>
    <published>2017-01-15T06:07:59.000Z</published>
    <updated>2017-01-15T07:17:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>우선 softmax란?<br>logistic을 지난번 포스팅에서 다뤘죠?<br>(<a href="https://penglover.github.io/2017/01/15/tensorflow-logisticC/">https://penglover.github.io/2017/01/15/tensorflow-logisticC/</a>)<br>그것을 일반화 한것이라고 보면 됩니다.<br>logistic은 계산을 해서 1, 0만 나누었다면<br>softmax는 같은 계산을 해서 각 class별로 확률을 매기죠.<br>가장 높은 확률의 편을 들어주는 것입니다.</p>
<p>텐서플로우가 아니라면 softmax 부분을 어떻게 구현해야 할까요?<br>일일히 class별로 weight와 bias를 계산해서 sigmoid 씌우고 주저리주저리…<br>어휴 참 끔찍합니다.<br>다행히 tensorflow에서는 메서드로 이를 제공하는데요.<br>아래의 코드는 weight와 bias를 행렬로 처리해서 W로 묶어서 두었습니다.<br>hypothesis, 즉 기대값은 X와 W의 행렬곱을 softmax처리 한 것입니다.<br>cost함수는 오류를 극대화 하기 위해서 + 선형성을 위해서 저렇게 한 것이라고 생각하면 됩니다.</p>
<p>softmax에 대해서 아는 분이라면 다음의 코드가 쉽게 이해가 가실겁니다.<br><figure class="highlight plain"><figcaption><span>input.txt</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#x0 x1 x2 y[A   B   C]</div><div class="line">1   2   1   0   0   1</div><div class="line">1   3   2   0   0   1</div><div class="line">1   3   4   0   0   1</div><div class="line">1   5   5   0   1   0</div><div class="line">1   7   5   0   1   0</div><div class="line">1   2   5   0   1   0</div><div class="line">1   6   6   1   0   0</div><div class="line">1   7   7   1   0   0</div></pre></td></tr></table></figure><br>아래는 예제 코드입니다.<br><figure class="highlight plain"><figcaption><span>softmax.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">xy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)</div><div class="line"></div><div class="line">x_data = np.transpose(xy[0:3])</div><div class="line">y_data = np.transpose(xy[3:])</div><div class="line"></div><div class="line">X = tf.placeholder(&quot;float&quot;, [None, 3])</div><div class="line">Y = tf.placeholder(&quot;float&quot;, [None, 3])</div><div class="line"></div><div class="line">W = tf.Variable(tf.zeros([3, 3]))</div><div class="line"></div><div class="line">hypothesis = tf.nn.softmax(tf.matmul(X, W))</div><div class="line"></div><div class="line">learning_rate = 0.01</div><div class="line"></div><div class="line">cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))</div><div class="line"></div><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</div><div class="line"></div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line">sess.run(init)</div><div class="line"></div><div class="line">for step in range(2001):</div><div class="line">    sess.run(optimizer, feed_dict=&#123;X: x_data, Y: y_data&#125;)</div><div class="line">    if step % 200 == 0:</div><div class="line">         print (step, sess.run(cost, feed_dict=&#123;X: x_data, Y: y_data&#125;), sess.run(W))</div><div class="line"></div><div class="line">a = sess.run(hypothesis, feed_dict=&#123;X: [[1, 11, 7]]&#125;)</div><div class="line">print (&quot;a :&quot;, a, sess.run(tf.arg_max(a, 1)))</div><div class="line"></div><div class="line">b = sess.run(hypothesis, feed_dict=&#123;X: [[1, 3, 4]]&#125;)</div><div class="line">print (&quot;b :&quot;, b, sess.run(tf.arg_max(b, 1)))</div><div class="line"></div><div class="line">c = sess.run(hypothesis, feed_dict=&#123;X: [[1, 1, 0]]&#125;)</div><div class="line">print (&quot;c :&quot;, c, sess.run(tf.arg_max(c, 1)))</div></pre></td></tr></table></figure><br><strong>결과값</strong><br>0 1.09048 [[-0.00083333  0.00041667  0.00041667]<br> [ 0.00166667  0.00291667 -0.00458333]<br> [ 0.00166667  0.00416667 -0.00583333]]<br>200 0.985653 [[-0.21679303 -0.05050437  0.26729742]<br> [ 0.02901031 -0.06265054  0.03364029]<br> [ 0.04244109  0.12451769 -0.16695869]]<br>400 0.926073 [[-0.41495511 -0.10318027  0.51813519]<br> [ 0.03762176 -0.10302337  0.06540174]<br> [ 0.07457665  0.17761268 -0.25218898]]<br>600 0.879342 [[-0.59596533 -0.1515553   0.74752003]<br> [ 0.04480708 -0.11983915  0.07503238]<br> [ 0.10447746  0.20644082 -0.31091776]]<br>800 0.840971 [[-0.76270223 -0.19499816  0.95770001]<br> [ 0.05223157 -0.12450957  0.07227841]<br> [ 0.13095778  0.22218271 -0.35314   ]]<br>1000 0.808647 [[-0.91752577 -0.2334879   1.15101326]<br> [ 0.06007884 -0.12292791  0.06284945]<br> [ 0.15438823  0.23068959 -0.38507724]]<br>1200 0.780959 [[-1.06231129 -0.26727253  1.32958329]<br> [ 0.06808005 -0.11823834  0.05015875]<br> [ 0.17550454  0.23514733 -0.41065112]]<br>1400 0.756943 [[-1.19854808 -0.29670808  1.49525583]<br> [ 0.07591439 -0.11214777  0.03623381]<br> [ 0.19498996  0.237331   -0.43232018]]<br>1600 0.735892 [[-1.32743537 -0.32218221  1.64961684]<br> [ 0.08333746 -0.10557999  0.022243  ]<br> [ 0.21336642  0.23823628 -0.45160189]]<br>1800 0.717269 [[-1.44994974 -0.34407791  1.79402602]<br> [ 0.09020081 -0.09902246  0.00882213]<br> [ 0.23099625  0.23841871 -0.46941414]]<br>2000 0.700649 [[-1.56689739 -0.36275655  1.92965221]<br> [ 0.09643649 -0.09271803 -0.00371792]<br> [ 0.24811605  0.23818412 -0.48629922]]<br>a : [[ 0.68849677  0.26731515  0.04418808]] [0]<br>b : [[ 0.24322268  0.44183081  0.3149465 ]] [1]<br>c : [[ 0.02974809  0.08208466  0.8881672 ]] [2]</p>
<p>정말 놀랍죠?<br>참고로 transpose는 열과 행을 바꾸는 것입니다.<br>reduction_indices=1 이부분도 뭔지 모르실수가 있는데 아래의 링크에 설명했습니다.<br><a href="https://penglover.github.io/2017/01/15/tensorflow-reduction-indices/">https://penglover.github.io/2017/01/15/tensorflow-reduction-indices/</a></p>
<p>softmax가 이해가 안가시는분들은 이걸봐주세요.<br>김성훈님의 강의입니다.<br><div class="video-container"><iframe src="//www.youtube.com/embed/MFAnsx1y9ZI" frameborder="0" allowfullscreen></iframe></div><br>cost가 이해가 안가시는 분들은 이걸봐주세요.<br>마찬가지로 김성훈 교수님의 강의입니다.<br><div class="video-container"><iframe src="//www.youtube.com/embed/jMU9G5WEtBc" frameborder="0" allowfullscreen></iframe></div></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;우선 softmax란?&lt;br&gt;logistic을 지난번 포스팅에서 다뤘죠?&lt;br&gt;(&lt;a href=&quot;https://penglover.github.io/2017/01/15/tensorflow-logisticC/&quot;&gt;https://penglover.git
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우에서 reduction_indices 속성은 무엇일까?</title>
    <link href="https://penglover.github.io/2017/01/15/tensorflow-reduction-indices/"/>
    <id>https://penglover.github.io/2017/01/15/tensorflow-reduction-indices/</id>
    <published>2017-01-15T05:29:16.000Z</published>
    <updated>2017-01-15T05:33:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>텐서플로우를 공부하다보면 reduction_indices 속성이 등장합니다.<br>reduce_mean이라든가 연산메서드에서 등장하는 속성인데요.<br><figure class="highlight plain"><figcaption><span>simple.txt</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&apos;x&apos; is [[1., 1.]</div><div class="line">        [2., 2.]]</div></pre></td></tr></table></figure><br>다음의 행렬이 있다고 해보겠습니다.<br>결과값은 어떻게 될까요?<br>tf.reduce_mean(x) ==&gt; 1.5<br>tf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]<br>tf.reduce_mean(x, 1) ==&gt; [1.,  2.]</p>
<p>자 아시겠죠?<br>아무 값도 주지 않으면 전부다 처리해버립니다.<br>0으로 하면 열끼리 처리하고 1로 하면 행끼리 처리합니다.<br>도움이 되셨길 바라며 포스팅 마무리하겠습니다~</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;텐서플로우를 공부하다보면 reduction_indices 속성이 등장합니다.&lt;br&gt;reduce_mean이라든가 연산메서드에서 등장하는 속성인데요.&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;figcaption&gt;&lt;span&gt;si
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우로 logistic classification 구현해보자</title>
    <link href="https://penglover.github.io/2017/01/15/tensorflow-logisticC/"/>
    <id>https://penglover.github.io/2017/01/15/tensorflow-logisticC/</id>
    <published>2017-01-15T04:19:22.000Z</published>
    <updated>2017-01-15T05:30:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>logistic classification은 다들알고 계시죠?<br>true인지 false인지를 판가름해봅시다.<br>실제로 양자택일의 상황이 올 경우가 굉장히 많지요.<br>자, 코드부터 보겠습니다.<br>input.txt는 다음과 같습니다.<br><figure class="highlight plain"><figcaption><span>input.txt</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">#x0 x1 x2 y</div><div class="line">1   2   1   0</div><div class="line">1   3   2   0</div><div class="line">1   3   5   0</div><div class="line">1   5   5   1</div><div class="line">1   7   5   1</div><div class="line">1   2   5   1</div></pre></td></tr></table></figure><br>코드부분 입니다.<br><figure class="highlight plain"><figcaption><span>logisticC.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">xy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)</div><div class="line">x_data = xy[0:-1]</div><div class="line">y_data = xy[-1]</div><div class="line"></div><div class="line">X = tf.placeholder(tf.float32)</div><div class="line">Y = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))</div><div class="line"></div><div class="line">h = tf.matmul(W, X)</div><div class="line">hypothesis = tf.div(1., 1. + tf.exp(-h))</div><div class="line"></div><div class="line">cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))</div><div class="line"></div><div class="line">a = tf.Variable(0.1)  # learning rate, alpha</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(a)</div><div class="line">train = optimizer.minimize(cost)  # goal is minimize cost</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line">for step in range(2001):</div><div class="line">    sess.run(train, feed_dict=&#123;X: x_data, Y: y_data&#125;)</div><div class="line">    if step % 200 == 0:</div><div class="line">        print (step, sess.run(cost, feed_dict=&#123;X: x_data, Y: y_data&#125;), sess.run(W))</div><div class="line"></div><div class="line">print (&apos;-----------------------------------------&apos;)</div><div class="line">print (sess.run(hypothesis, feed_dict=&#123;X: [[1], [2], [2]]&#125;) &gt; 0.5)</div><div class="line">print (sess.run(hypothesis, feed_dict=&#123;X: [[1], [5], [5]]&#125;) &gt; 0.5)</div><div class="line">print (sess.run(hypothesis, feed_dict=&#123;X: [[1, 1], [4, 0], [2, 10]]&#125;) &gt; 0.5)</div></pre></td></tr></table></figure></p>
<p><strong>결과값</strong><br>0 0.919001 [[ 0.8754766  -0.10433824  0.33632374]]<br>200 0.509501 [[-1.38300133  0.11766662  0.33868256]]<br>400 0.42692 [[-2.6384747   0.24237575  0.51069313]]<br>600 0.391719 [[-3.46154785  0.31578434  0.6304763 ]]<br>800 0.373097 [[-4.06127024  0.36348528  0.72200185]]<br>1000 0.361825 [[-4.52822399  0.39683989  0.79584837]]<br>1200 0.354366 [[-4.90821457  0.42141104  0.85761738]]<br>1400 0.34911 [[-5.22723818  0.44022152  0.91062367]]<br>1600 0.345229 [[-5.5013628   0.45505521  0.95699048]]<br>1800 0.342259 [[-5.74115181  0.46703169  0.99815571]]</p>
<h2 id="2000-0-339921-5-95390177-0-47688928-1-03513932"><a href="#2000-0-339921-5-95390177-0-47688928-1-03513932" class="headerlink" title="2000 0.339921 [[-5.95390177  0.47688928  1.03513932]]"></a>2000 0.339921 [[-5.95390177  0.47688928  1.03513932]]</h2><p>[[False]]<br>[[ True]]<br>[[False  True]]</p>
<p>주목해야할 부분은 cost함수 입니다.<br>왜 시그모이드 함수를 씌우고 log를 취했을까요?<br>0, 1의 값만을 갖는 true or false의 문제이기 때문인데요.<br>기존의 LinearRegression을 그대로 적용하면 최적화에 어려움이 있습니다.<br>어떤 어려움인지는 맨 아래 링크에 나와있구요.<br>그래서 sigmoid를 씌운 것입니다.</p>
<p>log를 취한 이유는 gradient descent 알고리즘의 최적화와 연관이 있는데요.<br>그냥 sigmoid 만 취하면 함수가 울퉁불퉁해져서 최적화가 힘듭니다.<br>때문에 log를 취해서 적용시키려고 하는 것이지요.<br>cost에 대해 모르시겠는 분들은 다음 동영상으로 학습해주세요.<br>김성훈 교수님의 강의입니다.<br><div class="video-container"><iframe src="//www.youtube.com/embed/6vzchGYEJBc" frameborder="0" allowfullscreen></iframe></div></p>
<p>기존의 LinearRegression 함수에 시그모이드 함수를 취했다는 것 이외에는 별 다른게 없죠?<br>하지만 sigmoid 함수의 필요성과 cost함수의 변화를 이해했다면 큰 발전입니다.<br>시그모이드 함수를 왜 취했는가를 모르신다면 아래의 url로 이동해주세요.<br><a href="https://penglover.github.io/2017/01/15/algorithm-sigmoid/">https://penglover.github.io/2017/01/15/algorithm-sigmoid/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;logistic classification은 다들알고 계시죠?&lt;br&gt;true인지 false인지를 판가름해봅시다.&lt;br&gt;실제로 양자택일의 상황이 올 경우가 굉장히 많지요.&lt;br&gt;자, 코드부터 보겠습니다.&lt;br&gt;input.txt는 다음과 같습니다.&lt;
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝에서는 왜 시그모이드함수를 쓰는지 알아보자</title>
    <link href="https://penglover.github.io/2017/01/15/algorithm-sigmoid/"/>
    <id>https://penglover.github.io/2017/01/15/algorithm-sigmoid/</id>
    <published>2017-01-15T03:27:36.000Z</published>
    <updated>2017-01-15T08:48:31.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="why-sigmoid"><a href="#why-sigmoid" class="headerlink" title="why sigmoid"></a>why sigmoid</h1><p>머신러닝 알고리즘을 공부하다가보면 시그모이드 함수를 마주쳤을 것입니다.<br>그런데 이게 진짜 과연 써야하는 것인가?<br>그냥 linear한 함수를 쓰면 안되나?<br>이런 의문을 바로 해결해준 유투브 강의가 있어서 소개합니다.<br>김성훈님의 강의입니다.<br><div class="video-container"><iframe src="//www.youtube.com/embed/PIjno6paszY" frameborder="0" allowfullscreen></iframe></div></p>
<p>Logistic classification을 설명해 주시면서 등장합니다.<br>이런 이유라면 충분히 시그모이드 함수의 등장이 이해가 가네요!</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;why-sigmoid&quot;&gt;&lt;a href=&quot;#why-sigmoid&quot; class=&quot;headerlink&quot; title=&quot;why sigmoid&quot;&gt;&lt;/a&gt;why sigmoid&lt;/h1&gt;&lt;p&gt;머신러닝 알고리즘을 공부하다가보면 시그모이드 함수를 마주쳤을 
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="algorithm" scheme="https://penglover.github.io/categories/ML/algorithm/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="algorithm" scheme="https://penglover.github.io/tags/algorithm/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우할 때 txt파일을 쉽게 배열로 가져오는법(numpy)</title>
    <link href="https://penglover.github.io/2017/01/15/tensorflow-numpy/"/>
    <id>https://penglover.github.io/2017/01/15/tensorflow-numpy/</id>
    <published>2017-01-15T02:50:55.000Z</published>
    <updated>2017-01-15T04:01:23.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h1><p>파이썬에 기본 내장된 라이브러리로 따로 설치할 필요가 없습니다.<br>텐서플로우를 활용하다가 손쉽게 txt파일을 배열로 바꾸기 위해 찾은 라이브러리입니다.<br>예를들어 다음의 텐서플로우 코드를 보겠습니다.<br><figure class="highlight plain"><figcaption><span>multiple_variables.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">xy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)</div><div class="line">x_data = xy[0:-1]</div><div class="line">y_data = xy[-1]</div><div class="line"></div><div class="line">W = tf.Variable(tf.random_uniform([1, len(x_data)], -1, 1))</div><div class="line">b = tf.Variable(tf.random_uniform([1], -1, 1))</div><div class="line"></div><div class="line">hypothesis = tf.matmul(W, x_data)</div><div class="line"></div><div class="line">cost = tf.reduce_mean(tf.square(hypothesis - y_data))</div><div class="line"></div><div class="line">a = tf.Variable(0.1)  # learning rate, alpha</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(a)</div><div class="line">train = optimizer.minimize(cost)  # goal is minimize cost</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div><div class="line">print(x_data)</div><div class="line">for step in range(2001):</div><div class="line">    sess.run(train)</div><div class="line">    if step % 200 == 0:</div><div class="line">        print (step, sess.run(cost), sess.run(W))</div><div class="line"></div></pre></td></tr></table></figure></p>
<p>input.txt는 다음과 같습니다.<br><figure class="highlight plain"><figcaption><span>input.txt</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#x0 x1 x2 y</div><div class="line">1   1   0   1</div><div class="line">1   0   2   2</div><div class="line">1   3   0   3</div><div class="line">1   0   4   4</div><div class="line">1   5   0   5</div></pre></td></tr></table></figure><br>numpy를 임포트 한 뒤에 loadtxt로 손쉽게 txt파일을 가져 올 수 있습니다.<br><figure class="highlight plain"><figcaption><span>numpy.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)</div></pre></td></tr></table></figure><br>그리고 다음과 같이 활용이 가능하지요.<br><figure class="highlight plain"><figcaption><span>numpy.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x_data = xy[0:-1]</div><div class="line">y_data = xy[-1]</div></pre></td></tr></table></figure></p>
<p>x_data는 그럼 다음과 같은 값으로 나오게 됩니다.<br>[[ 1.  1.  1.  1.  1.]<br> [ 1.  0.  3.  0.  5.]<br> [ 0.  2.  0.  4.  0.]]<br> 참 쉽죠?<br> 행과 열을 바꾸고 싶다면 transpose를 이용하면 쉽게 바꿀 수 있어요.<br> 자 그럼 이제부터 txt파일을 손쉽게 배열로 바꿔서 tensorflow에 적용할 수 있으실 것입니다.</p>
<p> 위의 코드 자체가 궁금하시다면<br> <a href="https://penglover.github.io/2017/01/15/how-to-handle-multiple-variables-in-tensorflow/">https://penglover.github.io/2017/01/15/how-to-handle-multiple-variables-in-tensorflow/</a><br> 이 포스팅을 봐주세요!</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;numpy&quot;&gt;&lt;a href=&quot;#numpy&quot; class=&quot;headerlink&quot; title=&quot;numpy&quot;&gt;&lt;/a&gt;numpy&lt;/h1&gt;&lt;p&gt;파이썬에 기본 내장된 라이브러리로 따로 설치할 필요가 없습니다.&lt;br&gt;텐서플로우를 활용하다가 손쉽게 tx
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우 LinearRegression 변수가 여러가지인경우 대처법</title>
    <link href="https://penglover.github.io/2017/01/15/how-to-handle-multiple-variables-in-tensorflow/"/>
    <id>https://penglover.github.io/2017/01/15/how-to-handle-multiple-variables-in-tensorflow/</id>
    <published>2017-01-15T02:28:27.000Z</published>
    <updated>2017-01-15T04:01:19.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="텐서플로우에서-선형회귀함수-적용중-변수가-여러개라면"><a href="#텐서플로우에서-선형회귀함수-적용중-변수가-여러개라면" class="headerlink" title="텐서플로우에서 선형회귀함수 적용중 변수가 여러개라면?"></a>텐서플로우에서 선형회귀함수 적용중 변수가 여러개라면?</h1><p>보통 우리는 hypothesis = a <em> w </em> x + b<br>이 모델을 기본으로 삼았습니다.<br>x가 input이고 hypothesis는 기대값이죠.<br>a는 learning rate이고 w는 weight b는 bias 입니다.</p>
<p>그런데 사실 어떤 요인에게 영향을 주는 요소가 한개일 경우가 잘 없죠?<br>두개 이상인 경우에는 어떻게 모델링을 할까요?<br>아주 간단하게 이런 경우를 모델링 할 수 있습니다.<br>hypothesis = a<em> (w1</em>x1 + w2*x2) + b<br>위 처럼 적용하면 되는데요. 코드를 살피겠습니다.<br>코드는 위의 hypothesis를 배열 형태로 구현하였습니다.<br>input 데이터는<br><figure class="highlight plain"><figcaption><span>input.txt</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#x0 x1 x2 y</div><div class="line">1   1   0   1</div><div class="line">1   0   2   2</div><div class="line">1   3   0   3</div><div class="line">1   0   4   4</div><div class="line">1   5   0   5</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><figcaption><span>multiple_variables.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">xy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)</div><div class="line">x_data = xy[0:-1]</div><div class="line">y_data = xy[-1]</div><div class="line"></div><div class="line">W = tf.Variable(tf.random_uniform([1, len(x_data)], -1, 1))</div><div class="line">b = tf.Variable(tf.random_uniform([1], -1, 1))</div><div class="line"></div><div class="line">hypothesis = tf.matmul(W, x_data)</div><div class="line"></div><div class="line">cost = tf.reduce_mean(tf.square(hypothesis - y_data))</div><div class="line"></div><div class="line">a = tf.Variable(0.1)  # learning rate, alpha</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(a)</div><div class="line">train = optimizer.minimize(cost)  # goal is minimize cost</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div><div class="line">print(x_data)</div><div class="line">for step in range(2001):</div><div class="line">    sess.run(train)</div><div class="line">    if step % 200 == 0:</div><div class="line">        print (step, sess.run(cost), sess.run(W))</div><div class="line"></div></pre></td></tr></table></figure>
<p><strong>결과값</strong><br>[[ 1.  1.  1.  1.  1.]<br> [ 1.  0.  3.  0.  5.]<br> [ 0.  2.  0.  4.  0.]]<br>0 0.43461 [[-0.20677651  1.29921687  1.09060895]]<br>200 5.33032e-08 [[ -5.47317148e-04   1.00014389e+00   1.00017071e+00]]<br>400 2.52953e-13 [[ -1.20747086e-06   1.00000036e+00   1.00000048e+00]]<br>600 1.77636e-14 [[ -2.96712130e-07   1.00000012e+00   1.00000012e+00]]<br>800 1.77636e-14 [[ -2.96712130e-07   1.00000012e+00   1.00000012e+00]]<br>1000 1.77636e-14 [[ -2.96712130e-07   1.00000012e+00   1.00000012e+00]]<br>1200 1.77636e-14 [[ -2.96712130e-07   1.00000012e+00   1.00000012e+00]]<br>1400 1.77636e-14 [[ -2.96712130e-07   1.00000012e+00   1.00000012e+00]]<br>1600 1.77636e-14 [[ -2.96712130e-07   1.00000012e+00   1.00000012e+00]]<br>1800 1.77636e-14 [[ -2.96712130e-07   1.00000012e+00   1.00000012e+00]]<br>2000 1.77636e-14 [[ -2.96712130e-07   1.00000012e+00   1.00000012e+00]]</p>
<p>간단하죠?<br>배열값은 순서대로 b, w1, w2입니다.<br>cost도 1.77636e-14로 거의 0에 가깝게 되었네요.<br>print(x_data)는 numpy 라이브러리가 어떤식으로 데이터를 가져와 주는지 알려주기 위해 넣어봤어요.<br>#부분은 무시를 한 채로 아래의 데이터를 행렬 형식으로 가져와줍니다.<br>numpy가 더 궁금하다면 다음 포스트를 봐주세요.</p>
<p>아주 쉽게 우리는 배열을 이용해서 weight를 추가시켰습니다.<br>질문이 있으면 댓글로 달아주세요!</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;텐서플로우에서-선형회귀함수-적용중-변수가-여러개라면&quot;&gt;&lt;a href=&quot;#텐서플로우에서-선형회귀함수-적용중-변수가-여러개라면&quot; class=&quot;headerlink&quot; title=&quot;텐서플로우에서 선형회귀함수 적용중 변수가 여러개라면?&quot;&gt;&lt;/a&gt;텐
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow에서 언제 LinearRegression을 적용시킬지 알아보는 법</title>
    <link href="https://penglover.github.io/2017/01/15/tensorflow-graph/"/>
    <id>https://penglover.github.io/2017/01/15/tensorflow-graph/</id>
    <published>2017-01-15T01:22:39.000Z</published>
    <updated>2017-01-15T04:00:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LinearRegression은-언제-적용시킬까요"><a href="#LinearRegression은-언제-적용시킬까요" class="headerlink" title="LinearRegression은 언제 적용시킬까요?"></a>LinearRegression은 언제 적용시킬까요?</h1><p>이것을 알기 위해서는 그래프를 그려 보는 것이 좋습니다.<br>파이썬에서는 그래프를 쉽게 그리게 해주는 라이브러리가 존재하는데요~<br>바로 matplotlib를 이용하면 아주 쉽습니다.<br>pip이 설치되어 있다면 그냥 pip install matplotlib 하시면 됩니다.<br>python3버전이신 경우에는 pip3 install matplotlib를 하시구요.<br><a href="http://matplotlib.org/users/installing.html" rel="external nofollow noopener noreferrer" target="_blank">http://matplotlib.org/users/installing.html</a> 에서 다른 설치법들도 알려줍니다~<br>설치 안되시면 댓글남겨주세요.</p>
<p>그럼 코드를 보면서 설명드리도록 하겠습니다.<br><figure class="highlight plain"><figcaption><span>matplotlib.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">X = [1., 2., 3.]</div><div class="line">Y = [1., 2., 3.]</div><div class="line">m = samples = len(X)</div><div class="line"></div><div class="line">W = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">hypothesis = tf.mul(X, W)</div><div class="line"></div><div class="line">cost = tf.reduce_sum(tf.pow(hypothesis-Y,2))/m</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line"></div><div class="line">W_val = []</div><div class="line">cost_val = []</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div><div class="line">for i in range(-30,50):</div><div class="line">    print (i*0.1, sess.run(cost, feed_dict=&#123;W: i*0.1&#125;))</div><div class="line">    W_val.append(i*0.1)</div><div class="line">    cost_val.append(sess.run(cost, feed_dict=&#123;W: i*0.1&#125;))</div><div class="line"></div><div class="line">plt.plot(W_val, cost_val, &apos;ro&apos;)</div><div class="line">plt.ylabel(&apos;cost&apos;)</div><div class="line">plt.xlabel(&apos;W&apos;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure><br>결과값은 그래프가 포함되다보니 스크린샷으로 보여드리겠습니다.<br><img src="https://penglover.github.io/images/matplotlib_sample.png" alt="matplotlib_sample"></p>
<p>자 그럼 본격적인 이야기를 해보겠습니다.<br>코드는 참 평범합니다. 그냥 hypothesis-Y에 제곱을 한 값을 평균을 내어 준 것인데요.<br>우리는 이미 X와 Y의 관계가 X * 1 = Y라는 것을 알고 있습니다.<br>X = [1, 2, 3]이고 Y = [1, 2, 3]이니 당연한 일이지요.<br>컴퓨터를 통해서 그래프를 그려보도록 하겠습니다.<br>사용법은 코드를 조금만 보시면 이해가 쉽게 가실겁니다.</p>
<p>그림처럼 저렇게 순탄한 아래쪽이 둥근모형이면 손쉽게 LinearRegression을 적용할 수 있습니다.<br>기울기 값이 줄어들거나 늘어나는 패턴이 일정하기 때문이지요.<br>수식으로 순간미분값(해당 점의 기울기값)이 0에 가까워 지도록 찾아주기만 하면 되닌까요.</p>
<p>아주 간단하죠?<br>요약하자면 연속하고 아래쪽 둥근부분이 저렇게 그림처럼 한부분만 있으면 됩니다.<br>3차원의 경우에도 마찬가지에요!</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;LinearRegression은-언제-적용시킬까요&quot;&gt;&lt;a href=&quot;#LinearRegression은-언제-적용시킬까요&quot; class=&quot;headerlink&quot; title=&quot;LinearRegression은 언제 적용시킬까요?&quot;&gt;&lt;/a&gt;Line
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우로 linear regression 구현 및 설명</title>
    <link href="https://penglover.github.io/2017/01/14/tf-LinearRegression/"/>
    <id>https://penglover.github.io/2017/01/14/tf-LinearRegression/</id>
    <published>2017-01-14T14:50:53.000Z</published>
    <updated>2017-01-15T04:00:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h1><p>Linear Regression에 대한 지식이 없으시다면 아래의 동영상을 보시는 것을 추천합니다.<br><a href="https://www.youtube.com/watch?v=GmtqOlPYB84" rel="external nofollow noopener noreferrer" target="_blank">https://www.youtube.com/watch?v=GmtqOlPYB84</a><br>이미 알고 계신다면 렛츠고!<br>아! 참고로 tensorflow 버전이나 python버전이 다르다면<br>tf.global_variables_initializer, print, range 부분에 수정이 필요합니다.<br>오류나면 댓글남겨주세요 ㅎㅎ 바로 알려드릴게요.</p>
<figure class="highlight plain"><figcaption><span>LinearRegression.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line">x_data = [1, 2, 3, 4]</div><div class="line">y_data = [2, 4, 6, 8]</div><div class="line"></div><div class="line">W = tf.Variable(tf.random_uniform([1], -10000, 10000))</div><div class="line">b = tf.Variable(tf.random_uniform([1], -10000, 10000))</div><div class="line"></div><div class="line">X = tf.placeholder(tf.float32)</div><div class="line">Y = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">hypothesis = W * X + b</div><div class="line"></div><div class="line">cost = tf.reduce_mean(tf.square(hypothesis - Y))</div><div class="line"></div><div class="line">a = tf.Variable(0.1)</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(a)</div><div class="line">train = optimizer.minimize(cost)</div><div class="line"></div><div class="line">init = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line">sess.run(init)</div><div class="line"></div><div class="line">for step in range(2001):</div><div class="line">    sess.run(train, feed_dict=&#123;X: x_data, Y: y_data&#125;)</div><div class="line">    if step % 200 == 0:</div><div class="line">        print (step, sess.run(cost, feed_dict=&#123;X: x_data, Y: y_data&#125;), sess.run(W), sess.run(b))</div><div class="line"></div><div class="line">print (sess.run(hypothesis, feed_dict=&#123;X: [5, 10]&#125;))</div><div class="line">print (sess.run(hypothesis, feed_dict=&#123;X: [2.5, 1.5]&#125;))</div></pre></td></tr></table></figure>
<p><strong>결과값</strong><br>0 1.17541e+07 [-3064.45288086] [ 7678.85205078]<br>200 53.5173 [-4.08845949] [ 17.90081406]<br>400 0.000280612 [ 1.98605835] [ 0.04099015]<br>600 1.46635e-09 [ 1.99996805] [  9.38054509e-05]<br>800 1.42109e-14 [ 1.99999988] [  4.28800519e-07]<br>1000 0.0 [ 2.] [  1.18856534e-07]<br>1200 0.0 [ 2.] [  1.18856534e-07]<br>1400 0.0 [ 2.] [  1.18856534e-07]<br>1600 0.0 [ 2.] [  1.18856534e-07]<br>1800 0.0 [ 2.] [  1.18856534e-07]<br>2000 0.0 [ 2.] [  1.18856534e-07]<br>[ 10.  20.]<br>[ 5.  3.]</p>
<p>아주 쉽게 이해가 가실 것입니다.<br>W와 b를 -10000에서 10000사이의 랜덤한 값으로 두었습니다.<br>X와 Y는 32bit의 float형 데이터로 선언해 두었지요.<br>hypothesis는 input값이 될 X에게 w와 b를 더해서 나오는 결과에 대한 기대값입니다.<br>cost는 기대값과 실제값을 뺀것을 제곱을 한 것의 평균값을 갖게 될 것입니다.<br>reduce_mean은 참고로 평균값을 내게 해주는 매소드입니다.<br>(m개의 input이 있으면 그것들의 평균값을 매겨줌)<br>a는 learning rate입니다.<br>tf.train.GradientDescentOptimizer가 gradient descent 알고리즘을 처리해줍니다.<br>그리고 minimize가 cost를 인자로 받아서 train될때마다 W와 b를 업데이트합니다.<br>화면이 너무 꽉 찰 것 같아서 step은 200개당 1번 print 했습니다.<br>결과값을 보니 training이 아주 잘 되었네요!</p>
<p>헷갈리는 tensorflow의 링크가 있다면 아래에 모든 것이 나와있습니다.<br><a href="https://www.tensorflow.org/versions/master/api_docs/python/math_ops/reduction#reduce_mean" rel="external nofollow noopener noreferrer" target="_blank">https://www.tensorflow.org/versions/master/api_docs/python/math_ops/reduction#reduce_mean</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Linear-Regression&quot;&gt;&lt;a href=&quot;#Linear-Regression&quot; class=&quot;headerlink&quot; title=&quot;Linear Regression&quot;&gt;&lt;/a&gt;Linear Regression&lt;/h1&gt;&lt;p&gt;Linear Reg
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우에서 placeholder란 무엇일까?</title>
    <link href="https://penglover.github.io/2017/01/14/tf-placeholder/"/>
    <id>https://penglover.github.io/2017/01/14/tf-placeholder/</id>
    <published>2017-01-14T13:01:16.000Z</published>
    <updated>2017-01-15T04:00:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>텐서플로우에서 참 재밌는 메서드가 있습니다.<br>바로 placeholder 인데요!<br>우리는 이것을 통해 값의 대입을 미룰 수 있습니다.<br>아래의 코드에서 자세히 확인해보겠습니다.<br>아! 참고로 python버전이 다르다면<br>print부분에 수정이 필요합니다.</p>
<figure class="highlight plain"><figcaption><span>placeholder.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line">a = tf.placeholder(tf.int16)</div><div class="line">b = tf.placeholder(tf.int16)</div><div class="line"></div><div class="line">add = tf.add(a, b)</div><div class="line">mul = tf.mul(a, b)</div><div class="line"></div><div class="line"></div><div class="line">print(&quot;더한 결과는 %i&quot; % sess.run(add,feed_dict=&#123;a: 2, b: 3&#125;))</div><div class="line">print(&quot;곱한 결과는 %i&quot; % sess.run(mul,feed_dict=&#123;a: 3, b: 4&#125;))</div></pre></td></tr></table></figure>
<p><strong>결과값</strong><br>더한 결과는 5<br>곱한 결과는 12</p>
<p>placeholder를 통해 먼저 16bit 크기의 int형 데이터의 공간을 만들어 놓았습니다.<br>그리고 나중에 연산을 할 때에 대입값을 지정해 주었지요.<br>나중에 아주 유용하게 많이 쓰이니 꼭 잘 숙지하고 넘어가야 하는 부분입니다.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;텐서플로우에서 참 재밌는 메서드가 있습니다.&lt;br&gt;바로 placeholder 인데요!&lt;br&gt;우리는 이것을 통해 값의 대입을 미룰 수 있습니다.&lt;br&gt;아래의 코드에서 자세히 확인해보겠습니다.&lt;br&gt;아! 참고로 python버전이 다르다면&lt;br&gt;pri
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우에서 상수란 존재하지 않는다? 모든것은 연산!</title>
    <link href="https://penglover.github.io/2017/01/14/tensorflow1/"/>
    <id>https://penglover.github.io/2017/01/14/tensorflow1/</id>
    <published>2017-01-14T12:33:46.000Z</published>
    <updated>2017-01-15T04:00:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="텐서플로우에서-상수란-존재하지-않는다"><a href="#텐서플로우에서-상수란-존재하지-않는다" class="headerlink" title="텐서플로우에서 상수란 존재하지 않는다"></a>텐서플로우에서 상수란 존재하지 않는다</h1><figure class="highlight plain"><figcaption><span>NoConstant.py</span></figcaption><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line"></div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line">a = tf.constant(1)</div><div class="line">b = tf.constant(2)</div><div class="line"></div><div class="line">c = a+b</div><div class="line"></div><div class="line">print(a)</div><div class="line">print(b)</div><div class="line">print(c)</div><div class="line"></div><div class="line">print(sess.run(a))</div><div class="line">print(sess.run(b))</div><div class="line">print(sess.run(c))</div></pre></td></tr></table></figure>
<p><strong>결과값</strong><br>Tensor(“Const:0”, shape=(), dtype=int32)<br>Tensor(“Const_1:0”, shape=(), dtype=int32)<br>Tensor(“add:0”, shape=(), dtype=int32)<br>1<br>2<br>3</p>
<p>위에 보이듯이 모든 텐서플로우에서의 변수는 상수상태로 존재하지 않습니다.<br><strong>operation 상태로 존재하고 session에 run메소드를 주는 순간 연산이 이루어지지요.</strong><br>이로서 우리는 각 노드들을 병렬적으로 다룰 수 있게 되지요.<br>한마디로 각 노드들은 tensor들(데이터 배열들)을 나르는 역할을 할 뿐이기 때문에 모든 node는 operation이라고 하는 것이지요.<br>아래의 애니메이션을 보면 더 정확히 아실 수 있으실거에요!<br><img src="https://camo.githubusercontent.com/4ee55154486232ec9edd8f1a3bad4c4a146f6cfe/68747470733a2f2f7777772e74656e736f72666c6f772e6f72672f696d616765732f74656e736f72735f666c6f77696e672e676966" alt="움직이는텐서플로우그래프"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;텐서플로우에서-상수란-존재하지-않는다&quot;&gt;&lt;a href=&quot;#텐서플로우에서-상수란-존재하지-않는다&quot; class=&quot;headerlink&quot; title=&quot;텐서플로우에서 상수란 존재하지 않는다&quot;&gt;&lt;/a&gt;텐서플로우에서 상수란 존재하지 않는다&lt;/h1&gt;&lt;
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우란 무엇일까? 추천 설치방법</title>
    <link href="https://penglover.github.io/2017/01/14/What-is-tensorflow-How-to-install/"/>
    <id>https://penglover.github.io/2017/01/14/What-is-tensorflow-How-to-install/</id>
    <published>2017-01-14T10:21:32.000Z</published>
    <updated>2017-01-15T03:34:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="텐서플로우란-무엇일까"><a href="#텐서플로우란-무엇일까" class="headerlink" title="텐서플로우란 무엇일까"></a>텐서플로우란 무엇일까</h1><p>요새 굉장히 핫하죠! 머신러닝, 딥러닝!<br>(머신러닝과 딥러닝에 대한 설명은 다음에 포스팅을 따로 하겠습니다)<br><strong>텐서플로우는 구글에서 발표한 머신러닝 오픈소스 라이브러리로 내부는 C++로 되어있고 여러 언어의 API를 제공합니다.</strong><br><strong>가장 문서화가 잘 되어있고 인기있는 API는 파이썬입니다.</strong><br>때문에 주로 사람들이 파이썬으로 개발을 하는데요!<br>그렇다면 왜 텐서플로우가 핫할까요?<br>그것은 바로 텐서플로우를 이용하면 짧은 시간에 강력한 머신러닝, 딥러닝 코드를 짤 수 있기 때문입니다.<br>테스트할 때에 쓰기도 좋고 상용 시스템을 만들 때에도 좋은 라이브러리입니다.</p>
<h1 id="텐서플로우는-왜-빠를까"><a href="#텐서플로우는-왜-빠를까" class="headerlink" title="텐서플로우는 왜 빠를까"></a>텐서플로우는 왜 빠를까</h1><p>텐서플로우가 빠른데에는 이유가 있습니다.<br><img src="https://penglover.github.io/images/tensorflow_howtowork.png" alt="텐서플로우작동방식"><br>위의 그림에서 보시다시피 각각의 node들이 연산을 병렬적으로 처리합니다.<br>내부코드가 C++라서 빠른데다가 병렬성이 좋아서 여기저기 잘 붙습니다.<br>즉 굉장히 유연하게 cpu, gpu 등의 환경에서 빠르게 처리될 수 있습니다.</p>
<h1 id="텐서플로우-설치방법"><a href="#텐서플로우-설치방법" class="headerlink" title="텐서플로우 설치방법"></a>텐서플로우 설치방법</h1><blockquote><p>아래의 링크에 설치 방법이 자세히 나와있습니다. 가장 추천하는 방법은 Virtualenv 설치입니다. 이유는 가상환경을 이용함으로써 파이썬의 버전을 독립적으로 지켜주기 때문입니다.</p>
<footer><strong>@텐서플로우설치</strong><cite><a href="https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/get_started/os_setup.html" rel="external nofollow noopener noreferrer" target="_blank">tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/get_started/os_setup.html</a></cite></footer></blockquote>
<p>혹시 설치방법에 문제가 생긴 경우에는 댓글로 문의를 주세요!</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;텐서플로우란-무엇일까&quot;&gt;&lt;a href=&quot;#텐서플로우란-무엇일까&quot; class=&quot;headerlink&quot; title=&quot;텐서플로우란 무엇일까&quot;&gt;&lt;/a&gt;텐서플로우란 무엇일까&lt;/h1&gt;&lt;p&gt;요새 굉장히 핫하죠! 머신러닝, 딥러닝!&lt;br&gt;(머신러닝과 딥
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>텐서플로우 무료강의 소개</title>
    <link href="https://penglover.github.io/2017/01/14/tensorflow-lecture-korean/"/>
    <id>https://penglover.github.io/2017/01/14/tensorflow-lecture-korean/</id>
    <published>2017-01-14T08:24:28.000Z</published>
    <updated>2017-01-15T04:00:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>머신러닝에서 가장 핫한 라이브러리중 하나죠! tensorflow!<br>그런데 아직 한글로 된 강의가 많이 없는 것이 현실입니다.<br><strong>홍콩 과학기술대학교의 김성훈 교수님이 tensorflow를 무료로 강의해주십니다.</strong><br>대상은 비전공자나 전공자 중에서 머신러닝 입문자들입니다. ㅎㅎ<br>정말 기초부터 차근차근 잘 알려주십니다.<br><a href="https://hunkim.github.io/ml/" rel="external nofollow noopener noreferrer" target="_blank">https://hunkim.github.io/ml/</a><br>다음의 링크를 타고 가시면 강의를 볼 수 있습니다~^^</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;머신러닝에서 가장 핫한 라이브러리중 하나죠! tensorflow!&lt;br&gt;그런데 아직 한글로 된 강의가 많이 없는 것이 현실입니다.&lt;br&gt;&lt;strong&gt;홍콩 과학기술대학교의 김성훈 교수님이 tensorflow를 무료로 강의해주십니다.&lt;/strong
    
    </summary>
    
      <category term="ML" scheme="https://penglover.github.io/categories/ML/"/>
    
      <category term="TensorFlow" scheme="https://penglover.github.io/categories/ML/TensorFlow/"/>
    
    
      <category term="machinelearning" scheme="https://penglover.github.io/tags/machinelearning/"/>
    
      <category term="tensorflow" scheme="https://penglover.github.io/tags/tensorflow/"/>
    
  </entry>
  
</feed>
