{"meta":{"title":"IT's IT Blog","subtitle":"Penglover's Software house","description":"Talk about software","author":"Myeongsoo Kim","url":"https://penglover.github.io"},"pages":[],"posts":[{"title":"sigmoid함수가 버려진 이유 - ReLU","slug":"relu-vs-sigmoid","date":"2017-01-15T12:53:25.000Z","updated":"2017-01-15T13:01:06.000Z","comments":true,"path":"2017/01/15/relu-vs-sigmoid/","link":"","permalink":"https://penglover.github.io/2017/01/15/relu-vs-sigmoid/","excerpt":"","text":"ReLU많은 분들이 아시듯이 머신러닝에서는 sigmoid함수를 써서 작업했었습니다.그 이유는 아래의 글에서 이미 다루었습니다.https://penglover.github.io/2017/01/15/algorithm-sigmoid/ 그런데 이 sigmoid는 사실 전에 머신러닝 붐을 약화시킨 주범입니다.바로 hidden layer 즉 층이 깊어지면 깊어질수록 정확성을 오히려 떨어뜨린다는 것입니다.sigmoid 특성상 마이너스 값을 0에 가깝게 만듭니다.따라서 관계가 깊어지면 깊어질수록 미분의 체인룰에 의해 sigmoid된 값들이 곱해질때모두다 0에 가까운 값에 수렴하게 되어 input 값이 최종값에 아무런 영향을 끼치지 못하는 사태가 벌어지게 됩니다.이를 위해 탄생한게 ReLU입니다.매우 심플한데요.0이하의 값이 들어오면 0을 출력하고 0이상의 값이 오면 비례함수로 그냥 내보내는 겁니다.감이 오시죠?실제로 이 방법으로 hidden layer를 늘렸을때 즉 딥러닝에서 엄청난 결과를 가져옵니다.감이 안오시는 분들은 아래의 김성훈 교수님의 강의를 들어보세요 ㅎㅎ","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"DeepLearning","slug":"ML/DeepLearning","permalink":"https://penglover.github.io/categories/ML/DeepLearning/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"}]},{"title":"백프로퍼게이션(역전파)에 대한 친절한 강의","slug":"backpropagation","date":"2017-01-15T11:32:11.000Z","updated":"2017-01-15T11:41:27.000Z","comments":true,"path":"2017/01/15/backpropagation/","link":"","permalink":"https://penglover.github.io/2017/01/15/backpropagation/","excerpt":"","text":"backpropagation역전파 알고리즘, 즉 백프로퍼게이션 알고리즘은 머신러닝, 딥러닝의 핵심으로 많이 쓰입니다.어떻게 뉴런들을 학습시킬 것이냐? 에 대한 해답을 던져준 것이기 때문이죠.쉽게 말하면 feed forward를 통해 결과값을 우선 얻습니다.그리곤 실제값과의 차이를 통해 backpropagation으로 값들을 update시키죠.각 값들을 update시키는 기준은 무엇일까요?바로 미분을 통해서입니다.미분을 아는분들은 감이 오실겁니다.미분은 해당 값에서의 변화량이지요.쉽게 말해 전체 알고리즘을 f라고 두고 해당점을 a라고 둔다면?f를 a에 대해 미분하면 순간변화량, 즉 a가 f에 미치는 영향을 알 수 있습니다.이에 따라서 값을 매기가 update 자료로 삼지요.아래는 김성훈 교수님의 아주 친절한 강의입니다. 미분을 잘 모르는 분들을 위해서도 강의를 찍으셨습니다.","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"algorithm","slug":"ML/algorithm","permalink":"https://penglover.github.io/categories/ML/algorithm/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"algorithm","slug":"algorithm","permalink":"https://penglover.github.io/tags/algorithm/"}]},{"title":"딥러닝이란 무엇일까? 김성훈 교수님의 말씀!","slug":"what-is-deep-learning","date":"2017-01-15T10:09:31.000Z","updated":"2017-01-15T10:30:03.000Z","comments":true,"path":"2017/01/15/what-is-deep-learning/","link":"","permalink":"https://penglover.github.io/2017/01/15/what-is-deep-learning/","excerpt":"","text":"딥러닝이란?딥러닝이란 용어가 최근들어 많이 쓰이고 있습니다.머신 러닝의 일종으로 간단한 learning 구조를 쌓아 올려가며 순차적으로 학습하는 계층적 구조의 학습법이라는 정의가 있기는 하지만 잘 와닿지가 않죠?김성훈 교수님의 강의를 공유합니다. 딥러닝이란(상)딥러닝이란(하) http://hunkim.github.io/ml/ 에 가면 더 많은 강의들이 있습니다.","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"DeepLearning","slug":"ML/DeepLearning","permalink":"https://penglover.github.io/categories/ML/DeepLearning/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"}]},{"title":"머신러닝 코드 최적화하기 by rate, overfitting, regularization","slug":"how-can-upgrade-mlcode","date":"2017-01-15T08:07:38.000Z","updated":"2017-01-15T10:11:44.000Z","comments":true,"path":"2017/01/15/how-can-upgrade-mlcode/","link":"","permalink":"https://penglover.github.io/2017/01/15/how-can-upgrade-mlcode/","excerpt":"","text":"머신러닝 코드 최적화하기rate항상 우리는 머신러닝 코드를 짤때 learning rate라는 값을 줍니다.gradient descent알고리즘을 쓸 때에 업데이트 정도를 조절해주는데요.우리는 이 learning rate 를 조절함으로써 코드를 최적화 할 수 있습니다.너무 작은값을 주면 어떻게 될까요?또는 너무 큰값을 주면 어떻게 될까요?위의 그림처럼 되어서 최적화가 안됩니다.해결방안은?print로 꾸준히 cost함수의 변화를 체크하는 것이 최선입니다. overfittingoverfitting이라 하면 주로 test data가 적어서 일어납니다.무리하게 적은 데이터로 fitting을 하려다보니 이상해 지는 것이지요.또는 weight간의 값이 편차가 심하거나 해도 마찬가지로 일어납니다.그래프가 구부러져 보인다고 해서 ‘구부러졌다’라고 표현하기도 합니다.표준정규분포를 혹시 아시나요?z = (x-평균)/표준편차고등학교 수학시간에 한번 보셨을 겁니다.이것을 이용해서 weight간의 격차를 일반화해줌으로서 해결하는 경우가 대부분입니다.또는 regularization을 통해 해결할 수 있는데요. regularization기존의 우리가 cost를 구하는 개념에서 weight의 제곱을 또 추가한것을 구하는 개념입니다.gradient descent 알고리즘을 사용한다면 자동으로 weight의 제곱까지 고려해서 minimize를 시킬 것이고 그에따라서 자동으로 weight의 값들도 하향 평준화 시켜서 값을 낮춤으로써 구부러짐을 방지해주지요.구부러짐이 뭐지 하는생각이 들 수 있습니다. 위에서 보이듯 람다값이 작을수록 weight 값을 신경쓰지 않겠다는 것입니다.","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"algorithm","slug":"ML/algorithm","permalink":"https://penglover.github.io/categories/ML/algorithm/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"algorithm","slug":"algorithm","permalink":"https://penglover.github.io/tags/algorithm/"}]},{"title":"텐서플로우에서 softmax 메서드를 사용해 봅시다!","slug":"tensorflow-softmaxC","date":"2017-01-15T06:07:59.000Z","updated":"2017-01-15T07:17:07.000Z","comments":true,"path":"2017/01/15/tensorflow-softmaxC/","link":"","permalink":"https://penglover.github.io/2017/01/15/tensorflow-softmaxC/","excerpt":"","text":"우선 softmax란?logistic을 지난번 포스팅에서 다뤘죠?(https://penglover.github.io/2017/01/15/tensorflow-logisticC/)그것을 일반화 한것이라고 보면 됩니다.logistic은 계산을 해서 1, 0만 나누었다면softmax는 같은 계산을 해서 각 class별로 확률을 매기죠.가장 높은 확률의 편을 들어주는 것입니다. 텐서플로우가 아니라면 softmax 부분을 어떻게 구현해야 할까요?일일히 class별로 weight와 bias를 계산해서 sigmoid 씌우고 주저리주저리…어휴 참 끔찍합니다.다행히 tensorflow에서는 메서드로 이를 제공하는데요.아래의 코드는 weight와 bias를 행렬로 처리해서 W로 묶어서 두었습니다.hypothesis, 즉 기대값은 X와 W의 행렬곱을 softmax처리 한 것입니다.cost함수는 오류를 극대화 하기 위해서 + 선형성을 위해서 저렇게 한 것이라고 생각하면 됩니다. softmax에 대해서 아는 분이라면 다음의 코드가 쉽게 이해가 가실겁니다.input.txt123456789#x0 x1 x2 y[A B C]1 2 1 0 0 11 3 2 0 0 11 3 4 0 0 11 5 5 0 1 01 7 5 0 1 01 2 5 0 1 01 6 6 1 0 01 7 7 1 0 0아래는 예제 코드입니다.softmax.py1234567891011121314151617181920212223242526272829303132333435363738394041import tensorflow as tfimport numpy as npxy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)x_data = np.transpose(xy[0:3])y_data = np.transpose(xy[3:])X = tf.placeholder(&quot;float&quot;, [None, 3])Y = tf.placeholder(&quot;float&quot;, [None, 3])W = tf.Variable(tf.zeros([3, 3]))hypothesis = tf.nn.softmax(tf.matmul(X, W))learning_rate = 0.01cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), reduction_indices=1))optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for step in range(2001): sess.run(optimizer, feed_dict=&#123;X: x_data, Y: y_data&#125;) if step % 200 == 0: print (step, sess.run(cost, feed_dict=&#123;X: x_data, Y: y_data&#125;), sess.run(W))a = sess.run(hypothesis, feed_dict=&#123;X: [[1, 11, 7]]&#125;)print (&quot;a :&quot;, a, sess.run(tf.arg_max(a, 1)))b = sess.run(hypothesis, feed_dict=&#123;X: [[1, 3, 4]]&#125;)print (&quot;b :&quot;, b, sess.run(tf.arg_max(b, 1)))c = sess.run(hypothesis, feed_dict=&#123;X: [[1, 1, 0]]&#125;)print (&quot;c :&quot;, c, sess.run(tf.arg_max(c, 1)))결과값0 1.09048 [[-0.00083333 0.00041667 0.00041667] [ 0.00166667 0.00291667 -0.00458333] [ 0.00166667 0.00416667 -0.00583333]]200 0.985653 [[-0.21679303 -0.05050437 0.26729742] [ 0.02901031 -0.06265054 0.03364029] [ 0.04244109 0.12451769 -0.16695869]]400 0.926073 [[-0.41495511 -0.10318027 0.51813519] [ 0.03762176 -0.10302337 0.06540174] [ 0.07457665 0.17761268 -0.25218898]]600 0.879342 [[-0.59596533 -0.1515553 0.74752003] [ 0.04480708 -0.11983915 0.07503238] [ 0.10447746 0.20644082 -0.31091776]]800 0.840971 [[-0.76270223 -0.19499816 0.95770001] [ 0.05223157 -0.12450957 0.07227841] [ 0.13095778 0.22218271 -0.35314 ]]1000 0.808647 [[-0.91752577 -0.2334879 1.15101326] [ 0.06007884 -0.12292791 0.06284945] [ 0.15438823 0.23068959 -0.38507724]]1200 0.780959 [[-1.06231129 -0.26727253 1.32958329] [ 0.06808005 -0.11823834 0.05015875] [ 0.17550454 0.23514733 -0.41065112]]1400 0.756943 [[-1.19854808 -0.29670808 1.49525583] [ 0.07591439 -0.11214777 0.03623381] [ 0.19498996 0.237331 -0.43232018]]1600 0.735892 [[-1.32743537 -0.32218221 1.64961684] [ 0.08333746 -0.10557999 0.022243 ] [ 0.21336642 0.23823628 -0.45160189]]1800 0.717269 [[-1.44994974 -0.34407791 1.79402602] [ 0.09020081 -0.09902246 0.00882213] [ 0.23099625 0.23841871 -0.46941414]]2000 0.700649 [[-1.56689739 -0.36275655 1.92965221] [ 0.09643649 -0.09271803 -0.00371792] [ 0.24811605 0.23818412 -0.48629922]]a : [[ 0.68849677 0.26731515 0.04418808]] [0]b : [[ 0.24322268 0.44183081 0.3149465 ]] [1]c : [[ 0.02974809 0.08208466 0.8881672 ]] [2] 정말 놀랍죠?참고로 transpose는 열과 행을 바꾸는 것입니다.reduction_indices=1 이부분도 뭔지 모르실수가 있는데 아래의 링크에 설명했습니다.https://penglover.github.io/2017/01/15/tensorflow-reduction-indices/ softmax가 이해가 안가시는분들은 이걸봐주세요.김성훈님의 강의입니다.cost가 이해가 안가시는 분들은 이걸봐주세요.마찬가지로 김성훈 교수님의 강의입니다.","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"텐서플로우에서 reduction_indices 속성은 무엇일까?","slug":"tensorflow-reduction-indices","date":"2017-01-15T05:29:16.000Z","updated":"2017-01-15T05:33:48.000Z","comments":true,"path":"2017/01/15/tensorflow-reduction-indices/","link":"","permalink":"https://penglover.github.io/2017/01/15/tensorflow-reduction-indices/","excerpt":"","text":"텐서플로우를 공부하다보면 reduction_indices 속성이 등장합니다.reduce_mean이라든가 연산메서드에서 등장하는 속성인데요.simple.txt12&apos;x&apos; is [[1., 1.] [2., 2.]]다음의 행렬이 있다고 해보겠습니다.결과값은 어떻게 될까요?tf.reduce_mean(x) ==&gt; 1.5tf.reduce_mean(x, 0) ==&gt; [1.5, 1.5]tf.reduce_mean(x, 1) ==&gt; [1., 2.] 자 아시겠죠?아무 값도 주지 않으면 전부다 처리해버립니다.0으로 하면 열끼리 처리하고 1로 하면 행끼리 처리합니다.도움이 되셨길 바라며 포스팅 마무리하겠습니다~","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"텐서플로우로 logistic classification 구현해보자","slug":"tensorflow-logisticC","date":"2017-01-15T04:19:22.000Z","updated":"2017-01-15T05:30:03.000Z","comments":true,"path":"2017/01/15/tensorflow-logisticC/","link":"","permalink":"https://penglover.github.io/2017/01/15/tensorflow-logisticC/","excerpt":"","text":"logistic classification은 다들알고 계시죠?true인지 false인지를 판가름해봅시다.실제로 양자택일의 상황이 올 경우가 굉장히 많지요.자, 코드부터 보겠습니다.input.txt는 다음과 같습니다.input.txt1234567#x0 x1 x2 y1 2 1 01 3 2 01 3 5 01 5 5 11 7 5 11 2 5 1코드부분 입니다.logisticC.py1234567891011121314151617181920212223242526272829303132333435import tensorflow as tfimport numpy as npxy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)x_data = xy[0:-1]y_data = xy[-1]X = tf.placeholder(tf.float32)Y = tf.placeholder(tf.float32)W = tf.Variable(tf.random_uniform([1, len(x_data)], -1.0, 1.0))h = tf.matmul(W, X)hypothesis = tf.div(1., 1. + tf.exp(-h))cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))a = tf.Variable(0.1) # learning rate, alphaoptimizer = tf.train.GradientDescentOptimizer(a)train = optimizer.minimize(cost) # goal is minimize costinit = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for step in range(2001): sess.run(train, feed_dict=&#123;X: x_data, Y: y_data&#125;) if step % 200 == 0: print (step, sess.run(cost, feed_dict=&#123;X: x_data, Y: y_data&#125;), sess.run(W))print (&apos;-----------------------------------------&apos;)print (sess.run(hypothesis, feed_dict=&#123;X: [[1], [2], [2]]&#125;) &gt; 0.5)print (sess.run(hypothesis, feed_dict=&#123;X: [[1], [5], [5]]&#125;) &gt; 0.5)print (sess.run(hypothesis, feed_dict=&#123;X: [[1, 1], [4, 0], [2, 10]]&#125;) &gt; 0.5) 결과값0 0.919001 [[ 0.8754766 -0.10433824 0.33632374]]200 0.509501 [[-1.38300133 0.11766662 0.33868256]]400 0.42692 [[-2.6384747 0.24237575 0.51069313]]600 0.391719 [[-3.46154785 0.31578434 0.6304763 ]]800 0.373097 [[-4.06127024 0.36348528 0.72200185]]1000 0.361825 [[-4.52822399 0.39683989 0.79584837]]1200 0.354366 [[-4.90821457 0.42141104 0.85761738]]1400 0.34911 [[-5.22723818 0.44022152 0.91062367]]1600 0.345229 [[-5.5013628 0.45505521 0.95699048]]1800 0.342259 [[-5.74115181 0.46703169 0.99815571]] 2000 0.339921 [[-5.95390177 0.47688928 1.03513932]][[False]][[ True]][[False True]] 주목해야할 부분은 cost함수 입니다.왜 시그모이드 함수를 씌우고 log를 취했을까요?0, 1의 값만을 갖는 true or false의 문제이기 때문인데요.기존의 LinearRegression을 그대로 적용하면 최적화에 어려움이 있습니다.어떤 어려움인지는 맨 아래 링크에 나와있구요.그래서 sigmoid를 씌운 것입니다. log를 취한 이유는 gradient descent 알고리즘의 최적화와 연관이 있는데요.그냥 sigmoid 만 취하면 함수가 울퉁불퉁해져서 최적화가 힘듭니다.때문에 log를 취해서 적용시키려고 하는 것이지요.cost에 대해 모르시겠는 분들은 다음 동영상으로 학습해주세요.김성훈 교수님의 강의입니다. 기존의 LinearRegression 함수에 시그모이드 함수를 취했다는 것 이외에는 별 다른게 없죠?하지만 sigmoid 함수의 필요성과 cost함수의 변화를 이해했다면 큰 발전입니다.시그모이드 함수를 왜 취했는가를 모르신다면 아래의 url로 이동해주세요.https://penglover.github.io/2017/01/15/algorithm-sigmoid/","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"머신러닝에서는 왜 시그모이드함수를 쓰는지 알아보자","slug":"algorithm-sigmoid","date":"2017-01-15T03:27:36.000Z","updated":"2017-01-15T08:48:31.000Z","comments":true,"path":"2017/01/15/algorithm-sigmoid/","link":"","permalink":"https://penglover.github.io/2017/01/15/algorithm-sigmoid/","excerpt":"","text":"why sigmoid머신러닝 알고리즘을 공부하다가보면 시그모이드 함수를 마주쳤을 것입니다.그런데 이게 진짜 과연 써야하는 것인가?그냥 linear한 함수를 쓰면 안되나?이런 의문을 바로 해결해준 유투브 강의가 있어서 소개합니다.김성훈님의 강의입니다. Logistic classification을 설명해 주시면서 등장합니다.이런 이유라면 충분히 시그모이드 함수의 등장이 이해가 가네요!","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"algorithm","slug":"ML/algorithm","permalink":"https://penglover.github.io/categories/ML/algorithm/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"algorithm","slug":"algorithm","permalink":"https://penglover.github.io/tags/algorithm/"}]},{"title":"텐서플로우할 때 txt파일을 쉽게 배열로 가져오는법(numpy)","slug":"tensorflow-numpy","date":"2017-01-15T02:50:55.000Z","updated":"2017-01-15T04:01:23.000Z","comments":true,"path":"2017/01/15/tensorflow-numpy/","link":"","permalink":"https://penglover.github.io/2017/01/15/tensorflow-numpy/","excerpt":"","text":"numpy파이썬에 기본 내장된 라이브러리로 따로 설치할 필요가 없습니다.텐서플로우를 활용하다가 손쉽게 txt파일을 배열로 바꾸기 위해 찾은 라이브러리입니다.예를들어 다음의 텐서플로우 코드를 보겠습니다.multiple_variables.py12345678910111213141516171819202122232425262728import tensorflow as tfimport numpy as npxy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)x_data = xy[0:-1]y_data = xy[-1]W = tf.Variable(tf.random_uniform([1, len(x_data)], -1, 1))b = tf.Variable(tf.random_uniform([1], -1, 1))hypothesis = tf.matmul(W, x_data)cost = tf.reduce_mean(tf.square(hypothesis - y_data))a = tf.Variable(0.1) # learning rate, alphaoptimizer = tf.train.GradientDescentOptimizer(a)train = optimizer.minimize(cost) # goal is minimize costinit = tf.global_variables_initializer()sess = tf.Session()sess.run(init)print(x_data)for step in range(2001): sess.run(train) if step % 200 == 0: print (step, sess.run(cost), sess.run(W)) input.txt는 다음과 같습니다.input.txt123456#x0 x1 x2 y1 1 0 11 0 2 21 3 0 31 0 4 41 5 0 5numpy를 임포트 한 뒤에 loadtxt로 손쉽게 txt파일을 가져 올 수 있습니다.numpy.py1xy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)그리고 다음과 같이 활용이 가능하지요.numpy.py12x_data = xy[0:-1]y_data = xy[-1] x_data는 그럼 다음과 같은 값으로 나오게 됩니다.[[ 1. 1. 1. 1. 1.] [ 1. 0. 3. 0. 5.] [ 0. 2. 0. 4. 0.]] 참 쉽죠? 행과 열을 바꾸고 싶다면 transpose를 이용하면 쉽게 바꿀 수 있어요. 자 그럼 이제부터 txt파일을 손쉽게 배열로 바꿔서 tensorflow에 적용할 수 있으실 것입니다. 위의 코드 자체가 궁금하시다면 https://penglover.github.io/2017/01/15/how-to-handle-multiple-variables-in-tensorflow/ 이 포스팅을 봐주세요!","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"텐서플로우 LinearRegression 변수가 여러가지인경우 대처법","slug":"how-to-handle-multiple-variables-in-tensorflow","date":"2017-01-15T02:28:27.000Z","updated":"2017-01-15T04:01:19.000Z","comments":true,"path":"2017/01/15/how-to-handle-multiple-variables-in-tensorflow/","link":"","permalink":"https://penglover.github.io/2017/01/15/how-to-handle-multiple-variables-in-tensorflow/","excerpt":"","text":"텐서플로우에서 선형회귀함수 적용중 변수가 여러개라면?보통 우리는 hypothesis = a w x + b이 모델을 기본으로 삼았습니다.x가 input이고 hypothesis는 기대값이죠.a는 learning rate이고 w는 weight b는 bias 입니다. 그런데 사실 어떤 요인에게 영향을 주는 요소가 한개일 경우가 잘 없죠?두개 이상인 경우에는 어떻게 모델링을 할까요?아주 간단하게 이런 경우를 모델링 할 수 있습니다.hypothesis = a (w1x1 + w2*x2) + b위 처럼 적용하면 되는데요. 코드를 살피겠습니다.코드는 위의 hypothesis를 배열 형태로 구현하였습니다.input 데이터는input.txt123456#x0 x1 x2 y1 1 0 11 0 2 21 3 0 31 0 4 41 5 0 5 multiple_variables.py12345678910111213141516171819202122232425262728import tensorflow as tfimport numpy as npxy = np.loadtxt(&apos;input.txt&apos;, unpack=True, dtype=&apos;float32&apos;)x_data = xy[0:-1]y_data = xy[-1]W = tf.Variable(tf.random_uniform([1, len(x_data)], -1, 1))b = tf.Variable(tf.random_uniform([1], -1, 1))hypothesis = tf.matmul(W, x_data)cost = tf.reduce_mean(tf.square(hypothesis - y_data))a = tf.Variable(0.1) # learning rate, alphaoptimizer = tf.train.GradientDescentOptimizer(a)train = optimizer.minimize(cost) # goal is minimize costinit = tf.global_variables_initializer()sess = tf.Session()sess.run(init)print(x_data)for step in range(2001): sess.run(train) if step % 200 == 0: print (step, sess.run(cost), sess.run(W)) 결과값[[ 1. 1. 1. 1. 1.] [ 1. 0. 3. 0. 5.] [ 0. 2. 0. 4. 0.]]0 0.43461 [[-0.20677651 1.29921687 1.09060895]]200 5.33032e-08 [[ -5.47317148e-04 1.00014389e+00 1.00017071e+00]]400 2.52953e-13 [[ -1.20747086e-06 1.00000036e+00 1.00000048e+00]]600 1.77636e-14 [[ -2.96712130e-07 1.00000012e+00 1.00000012e+00]]800 1.77636e-14 [[ -2.96712130e-07 1.00000012e+00 1.00000012e+00]]1000 1.77636e-14 [[ -2.96712130e-07 1.00000012e+00 1.00000012e+00]]1200 1.77636e-14 [[ -2.96712130e-07 1.00000012e+00 1.00000012e+00]]1400 1.77636e-14 [[ -2.96712130e-07 1.00000012e+00 1.00000012e+00]]1600 1.77636e-14 [[ -2.96712130e-07 1.00000012e+00 1.00000012e+00]]1800 1.77636e-14 [[ -2.96712130e-07 1.00000012e+00 1.00000012e+00]]2000 1.77636e-14 [[ -2.96712130e-07 1.00000012e+00 1.00000012e+00]] 간단하죠?배열값은 순서대로 b, w1, w2입니다.cost도 1.77636e-14로 거의 0에 가깝게 되었네요.print(x_data)는 numpy 라이브러리가 어떤식으로 데이터를 가져와 주는지 알려주기 위해 넣어봤어요.#부분은 무시를 한 채로 아래의 데이터를 행렬 형식으로 가져와줍니다.numpy가 더 궁금하다면 다음 포스트를 봐주세요. 아주 쉽게 우리는 배열을 이용해서 weight를 추가시켰습니다.질문이 있으면 댓글로 달아주세요!","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"tensorflow에서 언제 LinearRegression을 적용시킬지 알아보는 법","slug":"tensorflow-graph","date":"2017-01-15T01:22:39.000Z","updated":"2017-01-15T04:00:15.000Z","comments":true,"path":"2017/01/15/tensorflow-graph/","link":"","permalink":"https://penglover.github.io/2017/01/15/tensorflow-graph/","excerpt":"","text":"LinearRegression은 언제 적용시킬까요?이것을 알기 위해서는 그래프를 그려 보는 것이 좋습니다.파이썬에서는 그래프를 쉽게 그리게 해주는 라이브러리가 존재하는데요~바로 matplotlib를 이용하면 아주 쉽습니다.pip이 설치되어 있다면 그냥 pip install matplotlib 하시면 됩니다.python3버전이신 경우에는 pip3 install matplotlib를 하시구요.http://matplotlib.org/users/installing.html 에서 다른 설치법들도 알려줍니다~설치 안되시면 댓글남겨주세요. 그럼 코드를 보면서 설명드리도록 하겠습니다.matplotlib.py123456789101112131415161718192021222324252627282930import tensorflow as tfimport matplotlib.pyplot as pltX = [1., 2., 3.]Y = [1., 2., 3.]m = samples = len(X)W = tf.placeholder(tf.float32)hypothesis = tf.mul(X, W)cost = tf.reduce_sum(tf.pow(hypothesis-Y,2))/minit = tf.global_variables_initializer()W_val = []cost_val = []sess = tf.Session()sess.run(init)for i in range(-30,50): print (i*0.1, sess.run(cost, feed_dict=&#123;W: i*0.1&#125;)) W_val.append(i*0.1) cost_val.append(sess.run(cost, feed_dict=&#123;W: i*0.1&#125;))plt.plot(W_val, cost_val, &apos;ro&apos;)plt.ylabel(&apos;cost&apos;)plt.xlabel(&apos;W&apos;)plt.show()결과값은 그래프가 포함되다보니 스크린샷으로 보여드리겠습니다. 자 그럼 본격적인 이야기를 해보겠습니다.코드는 참 평범합니다. 그냥 hypothesis-Y에 제곱을 한 값을 평균을 내어 준 것인데요.우리는 이미 X와 Y의 관계가 X * 1 = Y라는 것을 알고 있습니다.X = [1, 2, 3]이고 Y = [1, 2, 3]이니 당연한 일이지요.컴퓨터를 통해서 그래프를 그려보도록 하겠습니다.사용법은 코드를 조금만 보시면 이해가 쉽게 가실겁니다. 그림처럼 저렇게 순탄한 아래쪽이 둥근모형이면 손쉽게 LinearRegression을 적용할 수 있습니다.기울기 값이 줄어들거나 늘어나는 패턴이 일정하기 때문이지요.수식으로 순간미분값(해당 점의 기울기값)이 0에 가까워 지도록 찾아주기만 하면 되닌까요. 아주 간단하죠?요약하자면 연속하고 아래쪽 둥근부분이 저렇게 그림처럼 한부분만 있으면 됩니다.3차원의 경우에도 마찬가지에요!","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"텐서플로우로 linear regression 구현 및 설명","slug":"tf-LinearRegression","date":"2017-01-14T14:50:53.000Z","updated":"2017-01-15T04:00:17.000Z","comments":true,"path":"2017/01/14/tf-LinearRegression/","link":"","permalink":"https://penglover.github.io/2017/01/14/tf-LinearRegression/","excerpt":"","text":"Linear RegressionLinear Regression에 대한 지식이 없으시다면 아래의 동영상을 보시는 것을 추천합니다.https://www.youtube.com/watch?v=GmtqOlPYB84이미 알고 계신다면 렛츠고!아! 참고로 tensorflow 버전이나 python버전이 다르다면tf.global_variables_initializer, print, range 부분에 수정이 필요합니다.오류나면 댓글남겨주세요 ㅎㅎ 바로 알려드릴게요. LinearRegression.py12345678910111213141516171819202122232425262728293031import tensorflow as tfx_data = [1, 2, 3, 4]y_data = [2, 4, 6, 8]W = tf.Variable(tf.random_uniform([1], -10000, 10000))b = tf.Variable(tf.random_uniform([1], -10000, 10000))X = tf.placeholder(tf.float32)Y = tf.placeholder(tf.float32)hypothesis = W * X + bcost = tf.reduce_mean(tf.square(hypothesis - Y))a = tf.Variable(0.1)optimizer = tf.train.GradientDescentOptimizer(a)train = optimizer.minimize(cost)init = tf.global_variables_initializer()sess = tf.Session()sess.run(init)for step in range(2001): sess.run(train, feed_dict=&#123;X: x_data, Y: y_data&#125;) if step % 200 == 0: print (step, sess.run(cost, feed_dict=&#123;X: x_data, Y: y_data&#125;), sess.run(W), sess.run(b))print (sess.run(hypothesis, feed_dict=&#123;X: [5, 10]&#125;))print (sess.run(hypothesis, feed_dict=&#123;X: [2.5, 1.5]&#125;)) 결과값0 1.17541e+07 [-3064.45288086] [ 7678.85205078]200 53.5173 [-4.08845949] [ 17.90081406]400 0.000280612 [ 1.98605835] [ 0.04099015]600 1.46635e-09 [ 1.99996805] [ 9.38054509e-05]800 1.42109e-14 [ 1.99999988] [ 4.28800519e-07]1000 0.0 [ 2.] [ 1.18856534e-07]1200 0.0 [ 2.] [ 1.18856534e-07]1400 0.0 [ 2.] [ 1.18856534e-07]1600 0.0 [ 2.] [ 1.18856534e-07]1800 0.0 [ 2.] [ 1.18856534e-07]2000 0.0 [ 2.] [ 1.18856534e-07][ 10. 20.][ 5. 3.] 아주 쉽게 이해가 가실 것입니다.W와 b를 -10000에서 10000사이의 랜덤한 값으로 두었습니다.X와 Y는 32bit의 float형 데이터로 선언해 두었지요.hypothesis는 input값이 될 X에게 w와 b를 더해서 나오는 결과에 대한 기대값입니다.cost는 기대값과 실제값을 뺀것을 제곱을 한 것의 평균값을 갖게 될 것입니다.reduce_mean은 참고로 평균값을 내게 해주는 매소드입니다.(m개의 input이 있으면 그것들의 평균값을 매겨줌)a는 learning rate입니다.tf.train.GradientDescentOptimizer가 gradient descent 알고리즘을 처리해줍니다.그리고 minimize가 cost를 인자로 받아서 train될때마다 W와 b를 업데이트합니다.화면이 너무 꽉 찰 것 같아서 step은 200개당 1번 print 했습니다.결과값을 보니 training이 아주 잘 되었네요! 헷갈리는 tensorflow의 링크가 있다면 아래에 모든 것이 나와있습니다.https://www.tensorflow.org/versions/master/api_docs/python/math_ops/reduction#reduce_mean","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"텐서플로우에서 placeholder란 무엇일까?","slug":"tf-placeholder","date":"2017-01-14T13:01:16.000Z","updated":"2017-01-15T04:00:19.000Z","comments":true,"path":"2017/01/14/tf-placeholder/","link":"","permalink":"https://penglover.github.io/2017/01/14/tf-placeholder/","excerpt":"","text":"텐서플로우에서 참 재밌는 메서드가 있습니다.바로 placeholder 인데요!우리는 이것을 통해 값의 대입을 미룰 수 있습니다.아래의 코드에서 자세히 확인해보겠습니다.아! 참고로 python버전이 다르다면print부분에 수정이 필요합니다. placeholder.py12345678910111213import tensorflow as tfsess = tf.Session()a = tf.placeholder(tf.int16)b = tf.placeholder(tf.int16)add = tf.add(a, b)mul = tf.mul(a, b)print(&quot;더한 결과는 %i&quot; % sess.run(add,feed_dict=&#123;a: 2, b: 3&#125;))print(&quot;곱한 결과는 %i&quot; % sess.run(mul,feed_dict=&#123;a: 3, b: 4&#125;)) 결과값더한 결과는 5곱한 결과는 12 placeholder를 통해 먼저 16bit 크기의 int형 데이터의 공간을 만들어 놓았습니다.그리고 나중에 연산을 할 때에 대입값을 지정해 주었지요.나중에 아주 유용하게 많이 쓰이니 꼭 잘 숙지하고 넘어가야 하는 부분입니다.","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"텐서플로우에서 상수란 존재하지 않는다? 모든것은 연산!","slug":"tensorflow1","date":"2017-01-14T12:33:46.000Z","updated":"2017-01-15T04:00:17.000Z","comments":true,"path":"2017/01/14/tensorflow1/","link":"","permalink":"https://penglover.github.io/2017/01/14/tensorflow1/","excerpt":"","text":"텐서플로우에서 상수란 존재하지 않는다NoConstant.py12345678910111213141516import tensorflow as tfsess = tf.Session()a = tf.constant(1)b = tf.constant(2)c = a+bprint(a)print(b)print(c)print(sess.run(a))print(sess.run(b))print(sess.run(c)) 결과값Tensor(“Const:0”, shape=(), dtype=int32)Tensor(“Const_1:0”, shape=(), dtype=int32)Tensor(“add:0”, shape=(), dtype=int32)123 위에 보이듯이 모든 텐서플로우에서의 변수는 상수상태로 존재하지 않습니다.operation 상태로 존재하고 session에 run메소드를 주는 순간 연산이 이루어지지요.이로서 우리는 각 노드들을 병렬적으로 다룰 수 있게 되지요.한마디로 각 노드들은 tensor들(데이터 배열들)을 나르는 역할을 할 뿐이기 때문에 모든 node는 operation이라고 하는 것이지요.아래의 애니메이션을 보면 더 정확히 아실 수 있으실거에요!","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"텐서플로우란 무엇일까? 추천 설치방법","slug":"What-is-tensorflow-How-to-install","date":"2017-01-14T10:21:32.000Z","updated":"2017-01-15T03:34:40.000Z","comments":true,"path":"2017/01/14/What-is-tensorflow-How-to-install/","link":"","permalink":"https://penglover.github.io/2017/01/14/What-is-tensorflow-How-to-install/","excerpt":"","text":"텐서플로우란 무엇일까요새 굉장히 핫하죠! 머신러닝, 딥러닝!(머신러닝과 딥러닝에 대한 설명은 다음에 포스팅을 따로 하겠습니다)텐서플로우는 구글에서 발표한 머신러닝 오픈소스 라이브러리로 내부는 C++로 되어있고 여러 언어의 API를 제공합니다.가장 문서화가 잘 되어있고 인기있는 API는 파이썬입니다.때문에 주로 사람들이 파이썬으로 개발을 하는데요!그렇다면 왜 텐서플로우가 핫할까요?그것은 바로 텐서플로우를 이용하면 짧은 시간에 강력한 머신러닝, 딥러닝 코드를 짤 수 있기 때문입니다.테스트할 때에 쓰기도 좋고 상용 시스템을 만들 때에도 좋은 라이브러리입니다. 텐서플로우는 왜 빠를까텐서플로우가 빠른데에는 이유가 있습니다.위의 그림에서 보시다시피 각각의 node들이 연산을 병렬적으로 처리합니다.내부코드가 C++라서 빠른데다가 병렬성이 좋아서 여기저기 잘 붙습니다.즉 굉장히 유연하게 cpu, gpu 등의 환경에서 빠르게 처리될 수 있습니다. 텐서플로우 설치방법아래의 링크에 설치 방법이 자세히 나와있습니다. 가장 추천하는 방법은 Virtualenv 설치입니다. 이유는 가상환경을 이용함으로써 파이썬의 버전을 독립적으로 지켜주기 때문입니다. @텐서플로우설치tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/get_started/os_setup.html 혹시 설치방법에 문제가 생긴 경우에는 댓글로 문의를 주세요!","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]},{"title":"텐서플로우 무료강의 소개","slug":"tensorflow-lecture-korean","date":"2017-01-14T08:24:28.000Z","updated":"2017-01-15T04:00:16.000Z","comments":true,"path":"2017/01/14/tensorflow-lecture-korean/","link":"","permalink":"https://penglover.github.io/2017/01/14/tensorflow-lecture-korean/","excerpt":"","text":"머신러닝에서 가장 핫한 라이브러리중 하나죠! tensorflow!그런데 아직 한글로 된 강의가 많이 없는 것이 현실입니다.홍콩 과학기술대학교의 김성훈 교수님이 tensorflow를 무료로 강의해주십니다.대상은 비전공자나 전공자 중에서 머신러닝 입문자들입니다. ㅎㅎ정말 기초부터 차근차근 잘 알려주십니다.https://hunkim.github.io/ml/다음의 링크를 타고 가시면 강의를 볼 수 있습니다~^^","categories":[{"name":"ML","slug":"ML","permalink":"https://penglover.github.io/categories/ML/"},{"name":"TensorFlow","slug":"ML/TensorFlow","permalink":"https://penglover.github.io/categories/ML/TensorFlow/"}],"tags":[{"name":"machinelearning","slug":"machinelearning","permalink":"https://penglover.github.io/tags/machinelearning/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://penglover.github.io/tags/tensorflow/"}]}]}